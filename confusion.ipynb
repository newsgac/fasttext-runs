{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution analysis\n",
    "\n",
    "Analyze the distributions of genre labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define code for normalizing an array and drawing a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "\n",
    "%matplotlib notebook\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def computeRowTotals(array):\n",
    "    rowTotals = []\n",
    "    for i in range(0,len(array)): rowTotals.append(sum(array[i]))\n",
    "    return(rowTotals)\n",
    "\n",
    "def computeColumnTotals(array):\n",
    "    columnTotals = []\n",
    "    if len(array) > 0: \n",
    "        for i in range(0,len(array[0])): columnTotals.append(sum([x[i] for x in array]))\n",
    "    return(columnTotals)\n",
    "    \n",
    "def normalizeArray(arrayIn):\n",
    "    arrayOut = []\n",
    "    for i in range(0,len(array)):\n",
    "        arrayOut.append([])\n",
    "        totalRow = sum(arrayIn[i])\n",
    "        for j in range(0,len(arrayIn[i])): arrayOut[i].append(round(100*arrayIn[i][j]/totalRow))\n",
    "    return(arrayOut)\n",
    "            \n",
    "def cm(array,columnNames):\n",
    "    arrayNorm = normalizeArray(array)\n",
    "    df_cm = pd.DataFrame(arrayNorm, index = columnNames, columns = columnNames)\n",
    "    plt.figure(figsize = (8,3))\n",
    "    sn.heatmap(df_cm, annot=True,cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional code for changing the background color of a Jupyter notebook cell. Use as \"%%bgc color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source https://stackoverflow.com/questions/49429585/\n",
    "#        how-to-change-the-background-color-of-a-single-cell-in-a-jupyter-notebook-jupy\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "@register_cell_magic\n",
    "def bgc(color,cell=None): \n",
    "    script = (\"var cell = this.closest('.code_cell');\" \n",
    "              \"var editor = cell.querySelector('.input_area');\" \n",
    "              \"editor.style.background='{}';\" \n",
    "              \"this.parentNode.removeChild(this)\" ).format(color) \n",
    "    display(HTML('<img src onerror=\"{}\">'.format(script)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next draw a confusion matrix for an analysis by FastText. The analysis was taken from the file 20190419.ipynb, heading \"Error analysis\", cell 2 (11cv training with 8331 articles, 9 labels, 20 epochs, with word vector dimension 300 and pretrained Wikipedia word vectors, accuracy=71.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = [\"NIE\",\"SER\",\"ACH\",\"VER\",\"REC\",\"OPI\",\"COL\",\"INT\",\"REP\"]\n",
    "array = [[3213,  104,  263,   69,   25,   12,    2,    9,    0 ],\n",
    "          [322, 1327,   76,   51,   54,   32,   34,   12,    6 ],\n",
    "          [412,   14,  745,   49,   21,   16,    6,   16,    2 ],\n",
    "          [128,   20,   79,  243,    5,    5,    1,    1,    0 ],\n",
    "           [12,    4,   32,    1,  246,    9,    2,    1,    0 ],\n",
    "           [16,    1,   99,    0,   10,   74,   12,    3,    0 ],\n",
    "            [7,   11,   36,    5,   11,   19,   81,    8,    6 ],\n",
    "            [5,    4,   61,    1,    2,    3,    9,   47,    1 ],\n",
    "            [2,    1,   75,    7,   11,    0,    9,    8,    5 ]]\n",
    "sum(computeRowTotals(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm(array,columnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swap labels**: put the labels together that are of the same class in the three-way split situation: NIE with INT, REP and VER, COL with ACH, OPI and REC, and SER by itself. The goal is to see if we can observe any confusion patterns within these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapRows(arrayIn,col1,col2):\n",
    "    arrayOut = []\n",
    "    for col in range(0,len(arrayIn)):\n",
    "        if col == col1: arrayOut.append(arrayIn[col2])\n",
    "        elif col == col2: arrayOut.append(arrayIn[col1])\n",
    "        else: arrayOut.append(arrayIn[col])\n",
    "    return(arrayOut)\n",
    "\n",
    "def swapColumns(arrayIn,col1,col2):\n",
    "    arrayOut = []\n",
    "    for col in range(0,len(arrayIn)):\n",
    "        row = list(arrayIn[col])\n",
    "        row[col1],row[col2] = row[col2],row[col1]\n",
    "        arrayOut.append(row)\n",
    "    return(arrayOut)\n",
    "\n",
    "def swapRowsColumns(arrayIn,columnNamesIn,col1,col2):\n",
    "    arrayTmp = swapRows(arrayIn,col1,col2)\n",
    "    arrayOut = swapColumns(arrayTmp,col1,col2)\n",
    "    columnNamesOut = swapRows(columnNamesIn,col1,col2)\n",
    "    return(arrayOut,columnNamesOut)\n",
    "\n",
    "def swapData(array,columnNames):\n",
    "    array1,columnNames1 = swapRowsColumns(array,columnNames,1,7)   # SER <=> INT\n",
    "    array2,columnNames2 = swapRowsColumns(array1,columnNames1,2,8) # ACH <=> REP\n",
    "    array3,columnNames3 = swapRowsColumns(array2,columnNames2,4,6) # REC <=> COL\n",
    "    array4,columnNames4 = swapRowsColumns(array3,columnNames3,5,8) # OPI <=> ACH\n",
    "    array5,columnNames5 = swapRowsColumns(array4,columnNames4,6,8) # REC <=> OPI\n",
    "    array6,columnNames6 = swapRowsColumns(array5,columnNames5,7,8) # SER <=> REC\n",
    "    return(array6,columnNames6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous confusion matrix with swapped columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arraySwapped,columnNamesSwapped = swapData(array,columnNames)\n",
    "cm(arraySwapped,columnNamesSwapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix for the same model but build from text with removed quoted text (heading \"Marking quoted text\", cell 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = [\"NIE\",\"SER\",\"ACH\",\"VER\",\"REC\",\"OPI\",\"COL\",\"INT\",\"REP\"]\n",
    "array = [[3186,  107,  278,   72,   29,   11,    2,   12,    0],\n",
    "          [314, 1314,   79,   53,   53,   35,   44,   13,    9],\n",
    "          [411,   14,  745,   50,   20,   13,    7,   19,    2],\n",
    "          [118,   18,   92,  240,    5,    5,    2,    1,    1],\n",
    "            [8,    6,   34,    1,  246,    8,    3,    1,    0],\n",
    "           [15,    2,   90,    1,   10,   81,   15,    1,    0],\n",
    "            [5,   10,   34,    8,   12,   20,   87,    4,    4],\n",
    "            [5,    2,   61,    0,    1,    1,   16,   45,    2],\n",
    "            [2,    1,   74,    6,   11,    0,   12,   10,    2]]\n",
    "arraySwapped,columnNamesSwapped = swapData(array,columnNames)\n",
    "sum(computeRowTotals(arraySwapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm(arraySwapped,columnNamesSwapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of this data is unknown. The numbers are for all data (9,246 articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = [\"ACH\",\"COL\",\"INT\",\"NIE\",\"OPI\",\"REC\",\"REP\",\"SER\",\"VER\"]\n",
    "array = [[796,   8,  17,  425,  21,  20,   1,   68,  54],\n",
    "          [38,  72,   9,    5,  22,  13,   3,   58,   3],\n",
    "          [78,  11,  56,    5,   3,   4,   1,   11,   1],\n",
    "         [301,   2,   6, 3375,  11,  18,   1,  257,  71],\n",
    "         [101,   8,   2,   15,  87,  12,   0,   22,   0],\n",
    "          [28,   3,   0,    8,  10, 286,   1,   43,   1],\n",
    "          [77,  11,  10,    2,   0,   7,  11,   14,   5],\n",
    "          [82,  27,  12,  318,  32,  43,   3, 1563,  47],\n",
    "          [84,   1,   0,  121,   3,   4,   0,   47, 250]]\n",
    "arraySwapped,columnNamesSwapped = swapData(array,columnNames)\n",
    "sum(computeRowTotals(arraySwapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm(array,columnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-estimation of label counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute sums of the rows give the true count of the labels while the absolute sum of the columns give the predicted counts. Note that the confusion matrices do not show absolute values but percentages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true distribution:     \",computeRowTotals(arraySwapped))\n",
    "print(\"predicted distribution:\",computeColumnTotals(arraySwapped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix contains information on how the machine learner has converted the true label distribution to the predicted label distribution. We can design a mathematical inverse of this operation. As a test, we apply it to the output of the machine learner for the training data. The result should be exactly the same as the true label distribution, which proves to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reEstimate(scoreList,confusionMatrix):\n",
    "    reEstimatedScoreList = []\n",
    "    columnTotals = computeColumnTotals(confusionMatrix) \n",
    "    for i in range(0,len(scoreList)):\n",
    "        total = 0.0\n",
    "        for column in range(0,len(scoreList)):\n",
    "            total += scoreList[column]*confusionMatrix[i][column]/columnTotals[column]\n",
    "        reEstimatedScoreList.append(total)\n",
    "    return(reEstimatedScoreList)\n",
    "\n",
    "print(\"re-estimated predicted distribution:\",reEstimate(computeColumnTotals(arraySwapped),arraySwapped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of this data is unknown. This data set contains 8,508 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first section with -w\n",
    "columnNames = [\"ACH\",\"COL\",\"INT\",\"NIE\",\"OPI\",\"REC\",\"REP\",\"SER\",\"VER\"]\n",
    "array = [[  773,   10,   17,  433,   15,   15,    2,   63,   47],\n",
    " [   34,   71,   10,    5,   21,   14,    2,   51,    4],\n",
    " [   77,   12,   53,    4,    3,    4,    1,    9,    1],\n",
    " [  294,    2,    6, 3091,    8,   18,    0,  231,   60],\n",
    " [  101,    9,    2,   12,   77,   12,    0,   20,    0],\n",
    " [   28,    1,    0,    6,   10,  242,    1,   44,    0],\n",
    " [   72,   11,    9,    3,    0,    8,    8,   12,    4],\n",
    " [   76,   22,   11,  271,   28,   39,    3, 1418,   41],\n",
    " [   82,    1,    0,  118,    5,    3,    0,   34,  203]]\n",
    "sum(computeRowTotals(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm(array,columnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [83,1,0,361,0,46,0,234,13]\n",
    "print(reEstimate(predictions,array))\n",
    "# correct: 35 11 6 332 14 48 10 218 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "DATADIR = \"/home/erikt/projects/newsgac/data-large/telegraaf.cwi\"\n",
    "FILENAMEPREFIX = \"TRAIN.\"\n",
    "FILENAMESUFFIX = \".labels\"\n",
    "EXPERIMENTS = [str(x) for x in list(range(0,10))+['A','B']]\n",
    "TRAIN = \"TRAIN\"\n",
    "TEST = \"TRAIN.20191004:14:29:50.labels\"\n",
    "\n",
    "def getFileSizes():\n",
    "    fileSizes = {}\n",
    "    for experiment in EXPERIMENTS:\n",
    "        inFileName = FILENAMEPREFIX+experiment+FILENAMESUFFIX\n",
    "        inFile = open(inFileName,\"r\")\n",
    "        lineCount = 0\n",
    "        for line in inFile: lineCount += 1\n",
    "        inFile.close()\n",
    "        fileSizes[experiment] = lineCount\n",
    "    return(fileSizes)\n",
    "\n",
    "def getLabels(inFileName):\n",
    "    labels = []\n",
    "    inFile = open(inFileName,\"r\")\n",
    "    for line in inFile:\n",
    "        label = re.sub(\"\\s.*$\",\"\",line)\n",
    "        labels.append(label)\n",
    "    inFile.close()\n",
    "    return(labels)\n",
    "\n",
    "def makeConfusionMatrix(experiment1):\n",
    "    index = 0\n",
    "    confusionMatrix = {}\n",
    "    predictions = []\n",
    "    goldLabelsOut = []\n",
    "    for experiment2 in EXPERIMENTS:\n",
    "        if experiment2 != experiment1:\n",
    "            inFileName = FILENAMEPREFIX+experiment2+\".\"+experiment1+FILENAMESUFFIX\n",
    "            inFile = open(inFileName,\"r\")\n",
    "            for line in inFile:\n",
    "                predictedLabel = re.sub(\"\\s.*$\",\"\",line)\n",
    "                goldLabel = goldLabels[index]\n",
    "                if not goldLabel in confusionMatrix: confusionMatrix[goldLabel] = {}\n",
    "                if not predictedLabel in confusionMatrix[goldLabel]: \n",
    "                    confusionMatrix[goldLabel][predictedLabel] = 0\n",
    "                confusionMatrix[goldLabel][predictedLabel] += 1\n",
    "                index += 1\n",
    "            inFile.close()\n",
    "        else:\n",
    "            inFileName = FILENAMEPREFIX+experiment1+FILENAMESUFFIX\n",
    "            inFile = open(inFileName,\"r\")\n",
    "            for line in inFile:\n",
    "                predictedLabel = re.sub(\"\\s.*$\",\"\",line)\n",
    "                predictions.append(predictedLabel)\n",
    "                goldLabelsOut.append(goldLabels[index])\n",
    "                index += 1\n",
    "            inFile.close()\n",
    "    return(confusionMatrix,predictions,goldLabelsOut)\n",
    "\n",
    "def computeMixtures(confusionMatrix):\n",
    "    mixtures = {}\n",
    "    for predictedLabel in confusionMatrix:\n",
    "        for goldLabel in confusionMatrix:\n",
    "            if not predictedLabel in confusionMatrix[goldLabel]:\n",
    "                confusionMatrix[goldLabel][predictedLabel] = 0\n",
    "        totalColumn = sum([confusionMatrix[goldLabel][predictedLabel] for goldLabel in confusionMatrix])\n",
    "        mixtures[predictedLabel] = {goldLabel:confusionMatrix[goldLabel][predictedLabel]/totalColumn \\\n",
    "                                   for goldLabel in confusionMatrix}\n",
    "    return(mixtures)\n",
    "\n",
    "def getMixtureAnalysis():\n",
    "    mixtureAnalysis = []\n",
    "    for experiment1 in EXPERIMENTS:\n",
    "        confusionMatrix,predictions,goldLabelsOut = makeConfusionMatrix(experiment1)\n",
    "        mixtures = computeMixtures(confusionMatrix)\n",
    "        for prediction in predictions:\n",
    "            mixtureAnalysis.append(mixtures[prediction])\n",
    "    return(mixtureAnalysis)\n",
    "\n",
    "def computeAccuracies(goldLabels,predictedLabels,mixtureAnalysis):\n",
    "    correct = 0.0\n",
    "    correctMixtures = 0.0\n",
    "    for i in range(0,len(goldLabels)):\n",
    "        correctMixtures += mixtureAnalysis[i][goldLabels[i]]\n",
    "        if goldLabels[i] == predictedLabels[i]: correct += 1\n",
    "    return(100*correct/len(goldLabels),100*correctMixtures/len(goldLabels))\n",
    "\n",
    "def computePredictedCounts(goldLabels,predictedLabels,mixtureAnalysis):\n",
    "    goldCounts = {}\n",
    "    predictedCounts = {}\n",
    "    predictedCountsMixtures = {}\n",
    "    for i in range(0,len(goldLabels)):\n",
    "        if not predictedLabels[i] in predictedCounts: predictedCounts[predictedLabels[i]] = 0\n",
    "        predictedCounts[predictedLabels[i]] += 1\n",
    "        for label in mixtureAnalysis[i]:\n",
    "            if not label in predictedCountsMixtures: predictedCountsMixtures[label] = 0\n",
    "            predictedCountsMixtures[label] += mixtureAnalysis[i][label]\n",
    "        if not goldLabels[i] in goldCounts: goldCounts[goldLabels[i]] = 0\n",
    "        goldCounts[goldLabels[i]] += 1\n",
    "    return(goldCounts,predictedCounts,predictedCountsMixtures)\n",
    "\n",
    "def printCounts(goldCounts,predictedCounts):\n",
    "    totalDeviance = 0.0\n",
    "    counts = 0\n",
    "    for label in sorted(predictedCounts.keys()): \n",
    "        deviance = int(0.5+abs((goldCounts[label]-predictedCounts[label])))\n",
    "        if predictedCounts[label] < goldCounts[label]: direction = \"-\"\n",
    "        else: direction = \"+\"\n",
    "        print(\"{} {} ({} {}{}%)\".format(label,int(0.5+predictedCounts[label]),\\\n",
    "                                        deviance,direction,int(0.5+100*deviance/goldCounts[label])))\n",
    "        counts += 1\n",
    "        totalDeviance += int(0.5+100*deviance/goldCounts[label])\n",
    "    print(\"Average deviance: {}%\\n\".format(int(0.5+totalDeviance/counts),\"%\"))\n",
    "    \n",
    "os.chdir(DATADIR)\n",
    "fileSizes = getFileSizes()\n",
    "goldLabels = getLabels(TRAIN)\n",
    "predictedLabels = getLabels(TEST)\n",
    "mixtureAnalysis = getMixtureAnalysis()\n",
    "accuracy,accuracyMixtures = computeAccuracies(goldLabels,predictedLabels,mixtureAnalysis)\n",
    "goldCounts,predictedCounts,predictedCountsMixtures = computePredictedCounts(goldLabels,predictedLabels,mixtureAnalysis)\n",
    "\n",
    "print(accuracy,accuracyMixtures)\n",
    "printCounts(goldCounts,predictedCounts)\n",
    "printCounts(goldCounts,predictedCountsMixtures)\n",
    "for label in sorted(goldCounts.keys()): print(label,goldCounts[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NEWSPAPERFILE=\"TRAIN.newspapers\"\n",
    "NEWSPAPERLIST=[\"05NRC_Handelsblad\",\"06De_Telegraaf\",\"08De_Volkskrant\"]\n",
    "\n",
    "newspapers = []\n",
    "inFile = open(NEWSPAPERFILE,\"r\")\n",
    "for line in inFile: newspapers.append(line.strip())\n",
    "inFile.close()\n",
    "goldCountsAll,forget1,forget2 = computePredictedCounts(goldLabels,predictedLabels,mixtureAnalysis)\n",
    "goldCountsAllTotal = sum(list(goldCountsAll.values()))\n",
    "for newspaper in NEWSPAPERLIST:\n",
    "    glN = [goldLabels[i] for i in range(0,len(newspapers)) if newspapers[i] == newspaper]\n",
    "    plN = [predictedLabels[i] for i in range(0,len(newspapers)) if newspapers[i] == newspaper]\n",
    "    maN = [mixtureAnalysis[i] for i in range(0,len(newspapers)) if newspapers[i] == newspaper]\n",
    "    goldCounts,predictedCounts,predictedCountsMixtures = computePredictedCounts(glN,plN,maN)\n",
    "    print(\"predictedCounts\",newspaper)\n",
    "    printCounts(goldCounts,predictedCounts)\n",
    "    predictedCountsTotal = sum(list(predictedCounts.values()))\n",
    "    baselineCounts = {x:int(0.5+goldCountsAll[x]*predictedCountsTotal/goldCountsAllTotal) for x in goldCountsAll}\n",
    "    print(\"baselineCounts\",newspaper)\n",
    "    printCounts(goldCounts,baselineCounts)\n",
    "    print(\"predictedCountsMixtures\",newspaper)\n",
    "    printCounts(goldCounts,predictedCountsMixtures)\n",
    "    print(\"goldCounts\",newspaper)\n",
    "    for label in sorted(goldCounts.keys()): print(label,goldCounts[label])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# antonio bella et al 2010 (applies to binary targets)\n",
    "\n",
    "def getMixtureAnalysis():\n",
    "    mixtureAnalysis = {}\n",
    "    for experiment1 in EXPERIMENTS:\n",
    "        confusionMatrix,predictions,goldLabels = makeConfusionMatrix(experiment1)\n",
    "        mixtures = computeMixtures(confusionMatrix)\n",
    "        mixtureAnalysis[experiment1] = {\"size\":0}\n",
    "        for key in mixtures: \n",
    "            mixtureAnalysis[experiment1][key] = {\"predicted\":0.0,\"estimated\":0,\"ppapCount\":0.0,\"ppanCount\":0.0}\n",
    "        for prediction in predictions:\n",
    "            mixtureAnalysis[experiment1][\"size\"] += 1\n",
    "            mixtureAnalysis[experiment1][prediction][\"predicted\"] += 1\n",
    "            for key in mixtures[prediction]: \n",
    "                mixtureAnalysis[experiment1][key][\"estimated\"] += mixtures[prediction][key]\n",
    "        for key1 in confusionMatrix:\n",
    "            totalRow = sum([confusionMatrix[key1][key2] for key2 in confusionMatrix])\n",
    "            mixtureAnalysis[experiment1][key1][\"ppapCount\"] = confusionMatrix[key1][key1]/totalRow\n",
    "            totalMatrix = sum([sum(confusionMatrix[key2].values()) for key2 in confusionMatrix if key2 != key1])\n",
    "            totalColumn = sum([confusionMatrix[key2][key1] for key2 in confusionMatrix if key2 != key1])\n",
    "            mixtureAnalysis[experiment1][key1][\"ppanCount\"] = totalColumn/totalMatrix\n",
    "    return(mixtureAnalysis)\n",
    "\n",
    "os.chdir(DATADIR)\n",
    "fileSizes = getFileSizes()\n",
    "goldLabels = getLabels(TRAIN)\n",
    "predictedLabels = getLabels(TEST)\n",
    "mixtureAnalysis = getMixtureAnalysis()\n",
    "counts = { label:0.0 for label in mixtureAnalysis['0'] if label != \"size\" }\n",
    "for experiment in mixtureAnalysis:\n",
    "    for label in mixtureAnalysis[experiment]: \n",
    "        if label != \"size\":\n",
    "            counts[label] += (mixtureAnalysis[experiment][label][\"estimated\"]/ \\\n",
    "                              mixtureAnalysis[experiment][\"size\"]- \\\n",
    "                              mixtureAnalysis[experiment][label][\"ppanCount\"])/ \\\n",
    "                             (mixtureAnalysis[experiment][label][\"ppapCount\"]- \\\n",
    "                              mixtureAnalysis[experiment][label][\"ppanCount\"])\n",
    "for key in counts: counts[key] *= len(goldLabels)/len(mixtureAnalysis)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
