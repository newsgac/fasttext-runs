{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext runs from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = \"/home/erikt/projects/newsgac/\"\n",
    "TRAINDIR = BASEDIR+\"data/\"\n",
    "PRETRAINEDDIR = BASEDIR+\"fasttext-runs/\"\n",
    "NEWSPAPERDIR = BASEDIR+\"fastText/\"\n",
    "TRAINFILENAME = \"link-articles-20190417-fasttext-train-1,2,7--collapsed9.txt\"\n",
    "TESTFILENAME = \"link-articles-20190417-fasttext-test-1,2,7--collapsed9.txt\"\n",
    "WIKIFILENAME = \"wiki.nl.vec\"\n",
    "NRCMODEL = \"nrc-model.vec\"\n",
    "VOLKSKRANTMODEL = \"volkskrant-model.vec\"\n",
    "METADATATEST = \"link-articles-20190417-fasttext-test-collapsed9.txt\"\n",
    "LABELCOLUMNID = 0\n",
    "NEWSPAPERCOLUMNID = 2\n",
    "ALL = \"ALL\"\n",
    "DIM = 300\n",
    "EPOCH = 50\n",
    "\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "    \n",
    "def readNewspapers(inFileName):\n",
    "    newspapers = []\n",
    "    inFile = open(inFileName,\"r\")\n",
    "    for line in inFile: \n",
    "        newspapers.append(line.split()[NEWSPAPERCOLUMNID])\n",
    "    inFile.close()\n",
    "    return(newspapers)\n",
    "\n",
    "def readTexts(inFileName,fromColumn=2):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    inFile = open(inFileName,\"r\")\n",
    "    for line in inFile: \n",
    "        words = line.strip().split()\n",
    "        labels.append(words[LABELCOLUMNID])\n",
    "        texts.append(\" \".join(words[fromColumn:]))\n",
    "    inFile.close()\n",
    "    return(labels,texts)\n",
    "\n",
    "def evaluate(newspapers,labels,predictedLabels):\n",
    "    correct = {}\n",
    "    total = {}\n",
    "    for i in range(0,len(labels)):\n",
    "        if not newspapers[i] in total: total[newspapers[i]] = 0\n",
    "        total[newspapers[i]] += 1\n",
    "        if predictedLabels[i] == labels[i]:\n",
    "            if not newspapers[i] in correct: correct[newspapers[i]] = 0\n",
    "            correct[newspapers[i]] += 1\n",
    "    for newspaper in total.keys():\n",
    "        print(\"{0:.3f}\".format(correct[newspaper]/total[newspaper]),newspaper)\n",
    "    print(\"{0:.3f}\".format(sum(correct.values())/sum(total.values())),ALL)\n",
    "\n",
    "labelsTest,textsTest = readTexts(TRAINDIR+TESTFILENAME)\n",
    "newspapersTest = readNewspapers(TRAINDIR+METADATATEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model, no dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t915\n",
      "P@1\t0.652\n",
      "R@1\t0.652\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(TRAINDIR+TRAINFILENAME,\\\n",
    "                                  dim=DIM,\n",
    "                                  epoch=100)\n",
    "print_results(*model.test(TRAINDIR+TESTFILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using dictionary from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t915\n",
      "P@1\t0.673\n",
      "R@1\t0.673\n"
     ]
    }
   ],
   "source": [
    "modelWiki = fasttext.train_supervised(TRAINDIR+TRAINFILENAME,\\\n",
    "                                      dim=DIM,\n",
    "                                      epoch=EPOCH,\\\n",
    "                                      pretrainedVectors=PRETRAINEDDIR+WIKIFILENAME)\n",
    "print_results(*modelWiki.test(TRAINDIR+TESTFILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using dictionary from Wikipedia and Volkskrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t915\n",
      "P@1\t0.671\n",
      "R@1\t0.671\n"
     ]
    }
   ],
   "source": [
    "modelVolkskrant = fasttext.train_supervised(TRAINDIR+TRAINFILENAME,\\\n",
    "                                  dim=DIM,\n",
    "                                  epoch=EPOCH,\\\n",
    "                                  pretrainedVectors=NEWSPAPERDIR+VOLKSKRANTMODEL)\n",
    "print_results(*modelVolkskrant.test(TRAINDIR+TESTFILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using dictionary from Wikipedia and NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t915\n",
      "P@1\t0.681\n",
      "R@1\t0.681\n"
     ]
    }
   ],
   "source": [
    "modelNRC = fasttext.train_supervised(TRAINDIR+TRAINFILENAME,\\\n",
    "                                  dim=DIM,\n",
    "                                  epoch=EPOCH,\\\n",
    "                                  pretrainedVectors=NEWSPAPERDIR+NRCMODEL)\n",
    "print_results(*modelNRC.test(TRAINDIR+TESTFILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsPredicted = model.predict(textsTest)\n",
    "labelsPredicted = [label[0] for label in labelsPredicted[0]]\n",
    "labelsPredictedWiki = modelWiki.predict(textsTest)\n",
    "labelsPredictedWiki = [label[0] for label in labelsPredictedWiki[0]]\n",
    "labelsPredictedVolkskrant = modelVolkskrant.predict(textsTest)\n",
    "labelsPredictedVolkskrant = [label[0] for label in labelsPredictedVolkskrant[0]]\n",
    "labelsPredictedNRC = modelNRC.predict(textsTest)\n",
    "labelsPredictedNRC = [label[0] for label in labelsPredictedNRC[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6524590163934426, 0.673224043715847, 0.6710382513661202, 0.6808743169398908)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = len([i for i in range(0,len(labelsTest)) \\\n",
    "               if labelsPredicted[i] == labelsTest[i]])\n",
    "correctWiki = len([i for i in range(0,len(labelsTest)) \\\n",
    "                   if labelsPredictedWiki[i] == labelsTest[i]])\n",
    "correctVolkskrant = len([i for i in range(0,len(labelsTest)) \\\n",
    "                        if labelsPredictedVolkskrant[i] == labelsTest[i]])\n",
    "correctNRC = len([i for i in range(0,len(labelsTest)) \\\n",
    "                        if labelsPredictedNRC[i] == labelsTest[i]])\n",
    "\n",
    "correct/len(labelsTest),correctWiki/len(labelsTest),correctVolkskrant/len(labelsTest),correctNRC/len(labelsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.638 NEWSPAPER=05NRC_Handelsblad\n",
      "0.673 NEWSPAPER=06De_Telegraaf\n",
      "0.639 NEWSPAPER=08De_Volkskrant\n",
      "0.652 ALL\n"
     ]
    }
   ],
   "source": [
    "evaluate(newspapersTest,labelsTest,labelsPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683 NEWSPAPER=05NRC_Handelsblad\n",
      "0.681 NEWSPAPER=06De_Telegraaf\n",
      "0.655 NEWSPAPER=08De_Volkskrant\n",
      "0.673 ALL\n"
     ]
    }
   ],
   "source": [
    "evaluate(newspapersTest,labelsTest,labelsPredictedWiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679 NEWSPAPER=05NRC_Handelsblad\n",
      "0.662 NEWSPAPER=06De_Telegraaf\n",
      "0.676 NEWSPAPER=08De_Volkskrant\n",
      "0.671 ALL\n"
     ]
    }
   ],
   "source": [
    "evaluate(newspapersTest,labelsTest,labelsPredictedVolkskrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695 NEWSPAPER=05NRC_Handelsblad\n",
      "0.665 NEWSPAPER=06De_Telegraaf\n",
      "0.689 NEWSPAPER=08De_Volkskrant\n",
      "0.681 ALL\n"
     ]
    }
   ],
   "source": [
    "evaluate(newspapersTest,labelsTest,labelsPredictedNRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Using word vectors from Wikipedia improves the overall score (0.652 &rarr; 0.673). Using pretrained word vectors from either Volkskrant or NRC texts also further improves the scores for these two newspapers (0.655 &rarr; 0.676 and 0.683 &rarr; 0.695).\n",
    "\n",
    "Caution: these conclusions are based on single runs of fastText, which uses random initializations so scores can be different in different runs with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
